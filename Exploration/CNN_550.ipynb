{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "bmI__ly7Mrsh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gVf-tUp4CqVh"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from google.colab import output#oven timer\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Load Data"
      ],
      "metadata": {
        "id": "q45ndmgQMt9O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(os.getcwd())\n",
        "#Path of the data folder\n",
        "#data_path = \"C:/Users/Admin/OneDrive - McGill University/COMP551550git/NLP-Authorship-Latin-Eng\\Data\"\n",
        "data_path = \"\"\n",
        "os.path.exists(data_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HH4Y32r1NLwq",
        "outputId": "3516045a-5647-4cc8-c0f8-5b3c424ef3cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load your CSV dataset\n",
        "training_data = pd.read_csv((data_path + \"translated_data_train_16k_format_fix.csv\"))\n",
        "validation_data = pd.read_csv((data_path + \"validation_data_filtered.csv\"))\n",
        "test_data = pd.read_csv((data_path + \"test_data_filtered.csv\"))\n",
        "\n",
        "train_en_x = training_data['en'].values\n",
        "train_la_x = training_data['la'].values\n",
        "valid_en_x = validation_data['en'].values\n",
        "valid_la_x = validation_data['la'].values\n",
        "test_en_x  = test_data['en'].values\n",
        "test_la_x  = test_data['la'].values\n",
        "\n",
        "train_y = training_data['file'].values\n",
        "valid_y = validation_data['file'].values\n",
        "test_y = test_data['file'].values\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = OneHotEncoder()\n",
        "label_encoder.fit(train_y.reshape(-1,1))\n",
        "\n",
        "train_y=label_encoder.transform(train_y.reshape(-1,1)).toarray()\n",
        "valid_y =label_encoder.transform(valid_y.reshape(-1,1)).toarray()\n",
        "test_y= label_encoder.transform(test_y.reshape(-1,1)).toarray()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l0jNzHnhIJxc",
        "outputId": "24cb7273-e7b0-4e01-fce0-8fff025be411"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'numpy.ndarray'>\n",
            "(363, 5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wwX-ymxiPEUX",
        "outputId": "9fde2789-5a2f-4e17-8b35-4332eb1130f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenize"
      ],
      "metadata": {
        "id": "ZOKyqjwNMwk6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(text_nump, tokenizer):\n"
      ],
      "metadata": {
        "id": "SeBB7nLWP-mn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define parameters\n",
        "max_len = 300  # maximum sequence length\n",
        "vocab_size = 128  # ASCII characters\n",
        "\n",
        "# Tokenize and pad the text sequences\n",
        "tokenizer_la = tf.keras.preprocessing.text.Tokenizer(char_level=True, filters=None, lower=False)\n",
        "tokenizer_la.fit_on_texts(train_la_x)\n",
        "\n",
        "tokenizer_en = tf.keras.preprocessing.text.Tokenizer(char_level=True, filters=None, lower=False)\n",
        "tokenizer_en.fit_on_texts(train_en_x)\n",
        "\n",
        "#CONTINUE HERE\n",
        "train_sequences_la = tokenizer.texts_to_sequences(train_texts)\n",
        "train_sequences_en =tokenizer.texts_to_sequences(test_texts)\n",
        "test_sequences =tokenizer.texts_to_sequences(test_texts)\n",
        "test_sequences_la =tokenizer.texts_to_sequences(test_texts)\n",
        "\n",
        "\n",
        "train_data = tf.keras.preprocessing.sequence.pad_sequences(train_sequences, maxlen=max_len)\n",
        "test_data = tf.keras.preprocessing.sequence.pad_sequences(test_sequences, maxlen=max_len)\n"
      ],
      "metadata": {
        "id": "l1hGUMSNMqyc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Build the model\n",
        "model = models.Sequential()\n",
        "model.add(layers.Embedding(input_dim=vocab_size, output_dim=16, input_length=max_len))\n",
        "model.add(layers.Conv1D(128, 5, activation='relu'))\n",
        "model.add(layers.GlobalMaxPooling1D())\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(layers.Dense(len(label_encoder.classes_), activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(train_data, train_labels, epochs=10, batch_size=64, validation_split=0.2)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_loss, test_accuracy = model.evaluate(test_data, test_labels)\n",
        "print(f\"Test Accuracy: {test_accuracy}\")"
      ],
      "metadata": {
        "id": "UtqN8rITQCub"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}